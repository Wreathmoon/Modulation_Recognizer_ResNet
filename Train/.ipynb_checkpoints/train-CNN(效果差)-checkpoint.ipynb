{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc={}\n",
    "Z_test = z_test.reshape((len(z_test)))\n",
    "SNRs = np.unique(Z_test)\n",
    "for snr in SNRs:\n",
    "    X_test_snr = x_test[Z_test==snr]\n",
    "    Y_test_snr = y_test[Z_test==snr]\n",
    "    \n",
    "    pre_Y_test = model.predict(X_test_snr)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,X_test_snr.shape[0]):    #\n",
    "        j = list(Y_test_snr[i,:]).index(1)   \n",
    "        classes = list(classes)\n",
    "        j = classes.index(classes[j])\n",
    "        k = int(np.argmax(pre_Y_test[i,:])) \n",
    "        k = classes.index(classes[k])\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "   \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    #print (\"Overall Accuracy %s: \"%snr, cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "    \n",
    "plt.plot(acc.keys(),acc.values())\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('SNR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9711c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import json\n",
    "\n",
    "from keras import metrics, regularizers, optimizers, backend\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, Flatten, pooling\n",
    "from keras.utils import np_utils, vis_utils\n",
    "\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d686e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(2018)\n",
    "rn.seed(2018)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=4)\n",
    "tf.random.set_seed(2018)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae98a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "seqLen = 128\n",
    "nClass = 12\n",
    "samNum = 100000 * nClass\n",
    "\n",
    "# load train data\n",
    "x_data_mat = h5py.File(r'C:\\Users\\xuluo\\Desktop\\Modulation\\Dataset\\Dataset\\train_data.mat')\n",
    "x_data_complex = x_data_mat['train_data']\n",
    "x_data_real = x_data_complex['real']\n",
    "x_data_imag = x_data_complex['imag']\n",
    "x_data_real = x_data_real.transpose()\n",
    "x_data_imag = x_data_imag.transpose()\n",
    "x_train = np.stack((x_data_real, x_data_imag), axis=1)\n",
    "\n",
    "y_data_mat = h5py.File(r'C:\\Users\\xuluo\\Desktop\\Modulation\\Dataset\\Dataset\\train_label.mat')\n",
    "y_data = y_data_mat['train_label']\n",
    "y_train = np_utils.to_categorical(y_data, nClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7a3d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200000, 2, 128)\n",
      "(1, 1200000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33eec092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data shuffle\n",
    "index = np.arange(y_train.shape[1])\n",
    "np.random.shuffle(index)\n",
    "x_train = x_train[index]\n",
    "y_train[0] = y_train[0][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73656783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200000, 2, 128)\n",
      "(1, 1200000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d346ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_in_ = Input(shape = (x_train.shape[1], x_train.shape[2], 1))\n",
    "ot = Conv2D(filters=64, kernel_size=(2,4), strides=1, padding='valid', use_bias=True, activation='relu')(_in_)\n",
    "ot = Conv2D(filters=16, kernel_size=(1,4), strides=1, padding='valid', use_bias=True, activation='relu')(ot)\n",
    "ot = Flatten()(ot)\n",
    "#ot = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(ot)\n",
    "ot = Dense(64, use_bias=True, activation='relu')(ot)\n",
    "ot = Dense(16, use_bias=True, activation='relu')(ot)\n",
    "_out_ = Dense(nClass, activation='softmax')(ot)\n",
    "model = Model(_in_, _out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90094c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2, 128, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 125, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 122, 16)        4112      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1952)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                124992    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 130,924\n",
      "Trainable params: 130,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#tensor_board = TensorBoard(log_dir='./tensorboard_log', histogram_freq=0, write_graph=True, write_images=False,\n",
    " #                           embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "               optimizer=adam, \n",
    "               metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a341dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14961/21600 [===================>..........] - ETA: 10s - loss: 1.6165 - categorical_accuracy: 0.3498"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train[0], \n",
    "          epochs=50, \n",
    "          batch_size=50,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True,\n",
    "          callbacks=[early_stopping])\n",
    "scores = model.evaluate(x_train, y_train[0])\n",
    "print(\" %s %f\" % (model.metrics_names[1], scores[1]))\n",
    "\n",
    "with open('model_struct.json', 'w') as f:\n",
    "    json.dump(model.to_json(), f)    \n",
    "model.save_weights('model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
